{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPc8d2xXx1enWbrXEbUm+Ri"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Ensemble Learning\n"," ## ASSIGNMENT :\n","\n"," Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n","behind it\n","\n","Answer 1 : **Ensemble Learning** is a machine learning technique where multiple individual models (often called \"base learners\" or \"weak learners\") are trained to solve the same problem and then combined to produce a single, more accurate, and robust prediction.\n","\n","**The key ideas behind why this works include:**\n","\n","* Error Correction: Different models make different types of mistakes. By combining them, the individual errors of one model are \"canceled out\" by the correct predictions of the others.\n","\n","* Improving Robustness: A single model might be overly sensitive to noise or outliers in the data. An ensemble is more stable because it relies on a consensus rather than a single viewpoint.\n","\n","* Diverse Perspectives: You can use different algorithms (e.g., combining a Decision Tree with a Support Vector Machine) or train the same algorithm on different subsets of data. This diversity ensures the ensemble captures a wider range of patterns.\n","\n","Question 2: What is the difference between Bagging and Boosting?\n","\n","Answer 2 : **1. How they handle the data**\n","\n","* Bagging: Imagine you have a large deck of cards. You deal out 10 different hands (subsets), allowing for some cards to repeat. You give each hand to a different person to analyze. No one’s work depends on anyone else’s.\n","\n","* Boosting: You give the whole deck to one person. After they finish, you look at the cards they struggled to identify. You then give a new hand to the next person, but this hand is heavily weighted with the cards the first person got wrong.\n","\n","**2. Bias vs. Variance**\n","\n","* Bagging is your go-to when you have a model that is too complex and \"overfits\" the training data (high variance). By averaging many such models, the random fluctuations cancel out, creating a smoother, more stable result.\n","+1\n","\n","* Boosting is best when your model is too simple and \"underfits\" (high bias). It keeps adding complexity step-by-step until the model can capture the underlying patterns of the data accurately.\n","\n","**3. Execution Speed**\n","\n","Because Bagging models are independent, you can train them all at once if you have multiple processors (Parallelization). Boosting must wait for the first model to finish before it can start the second, making it inherently slower to train.\n","\n","Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n","like Random Forest?\n","\n","Answer 3 : \"Bagging\" is actually a portmanteau of Bootstrap Aggregating. Bootstrap sampling is the \"engine\" that powers this method. Its primary roles are:\n","\n","* Creating Diversity: Since you sample with replacement, each bootstrap sample is slightly different. Some data points will appear multiple times, while others won't appear at all. This ensures that each base model (like a Decision Tree) sees a unique version of the truth.\n","+1\n","\n","* Reducing Correlation: In a Random Forest, if every tree was trained on the exact same data, they would all make the same predictions. Bootstrapping forces the trees to be different from one another, which is critical for the \"ensemble\" effect to work.\n","\n","* The \"63% Rule\": Mathematically, a bootstrap sample typically contains about 63.2% of the unique original data points. The remaining 36.8% are left out.\n","\n","Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n","evaluate ensemble models?\n","\n","Answer 4 : The OOB score is a clever way to evaluate the model's performance on \"unseen\" data without needing a separate test set or performing expensive Cross-Validation.3 Here is the step-by-step logic:\n","\n","1) Individual Predictions: For every row in your original dataset, the model identifies which specific trees did not use that row during their training.\n","\n","2) Consensus: Only those \"ignorant\" trees are allowed to vote on or predict the outcome for that row.\n","\n","3) Aggregated Error: This process is repeated for every single row in the dataset. The model then compares these \"OOB predictions\" to the actual true values.\n","\n","4) The Score: The final OOB score is the accuracy (for classification) or 6$R^2$ (for regression) of these aggregated predictions.\n","\n","Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n","Random Forest.\n","\n","Answer 5 : 1. The Stability Factor\n","\n","* In a single Decision Tree, the importance of a feature is highly dependent on the root node and early splits. If a specific feature is chosen for the first split, it \"steals\" the importance from other features that might have been just as good. If you remove even a few rows of data, the tree might choose a different root, causing the feature importance list to look completely different.\n","\n","* In a Random Forest, we use Bootstrap Sampling and Feature Randomness. This forces different trees to start with different features. By the time you average 500 trees, you get a much more \"democratic\" and stable view of which features actually matter across the entire dataset.\n","\n","2. Handling Correlated Features\n","\n","* Decision Tree: If you have two features, \"Temperature in Celsius\" and \"Temperature in Fahrenheit,\" a single tree will pick the one that gives the best first split. The other feature will then appear to have zero importance, even though it's perfectly predictive.\n","\n","* Random Forest: Because each split only considers a random subset of features, some trees will be forced to use Celsius and others to use Fahrenheit. When you average them, both features will show moderate-to-high importance, correctly identifying that both carry useful information.\n","\n","3. Common Bias:\n"," High Cardinality\n","It is important to note that both models share a common bias: they tend to favor high-cardinality features (features with many unique values, like IDs or zip codes). These features provide more potential \"split points,\" making it easier for the algorithm to reduce impurity by chance, which can artificially inflate their importance score.\n","\n","Question 6: Write a Python program to:\n","\n","● Load the Breast Cancer dataset using\n","sklearn.datasets.load_breast_cancer()\n","\n","● Train a Random Forest Classifier\n","\n","● Print the top 5 most important features based on feature importance scores.\n","\n"],"metadata":{"id":"QI5X8EqtuRav"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# 1. Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# 2. Train a Random Forest Classifier\n","# We set random_state for reproducibility\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X, y)\n","\n","# 3. Get feature importance scores\n","importances = rf_model.feature_importances_\n","\n","# Create a Series to map names to their importance scores\n","feature_series = pd.Series(importances, index=data.feature_names)\n","\n","# Sort and print the top 5\n","top_5_features = feature_series.sort_values(ascending=False).head(5)\n","\n","print(\"Top 5 Most Important Features in Breast Cancer Dataset:\")\n","print(\"-\" * 55)\n","print(top_5_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiC_Ukho1aPB","executionInfo":{"status":"ok","timestamp":1767762160341,"user_tz":-330,"elapsed":6292,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"75c4316a-4281-4cf7-dc18-221777cf39db"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 Most Important Features in Breast Cancer Dataset:\n","-------------------------------------------------------\n","worst area              0.139357\n","worst concave points    0.132225\n","mean concave points     0.107046\n","worst radius            0.082848\n","worst perimeter         0.080850\n","dtype: float64\n"]}]},{"cell_type":"markdown","source":["Question 7: Write a Python program to:\n","\n","● Train a Bagging Classifier using Decision Trees on the Iris dataset\n","\n","● Evaluate its accuracy and compare with a single Decision Tree\n","\n","Answer 7 : To compare a single Decision Tree against a Bagging ensemble, we use the BaggingClassifier from Scikit-Learn. In this example, the Bagging ensemble will use 100 separate Decision Trees, each trained on a different bootstrap sample of the Iris data."],"metadata":{"id":"hdRKaGJi1k0J"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# 2. Split into training and testing sets (80/20 split)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 3. Train a Single Decision Tree\n","tree_model = DecisionTreeClassifier(random_state=42)\n","tree_model.fit(X_train, y_train)\n","tree_pred = tree_model.predict(X_test)\n","tree_acc = accuracy_score(y_test, tree_pred)\n","\n","# 4. Train a Bagging Classifier using Decision Trees\n","# We use 100 trees (n_estimators=100)\n","bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(),\n","                                  n_estimators=100,\n","                                  random_state=42)\n","bagging_model.fit(X_train, y_train)\n","bagging_pred = bagging_model.predict(X_test)\n","bagging_acc = accuracy_score(y_test, bagging_pred)\n","\n","# 5. Compare Results\n","print(f\"Single Decision Tree Accuracy: {tree_acc:.4f}\")\n","print(f\"Bagging Classifier Accuracy:   {bagging_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZxeHN7m1y3F","executionInfo":{"status":"ok","timestamp":1767762270148,"user_tz":-330,"elapsed":477,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"96399ed9-5184-4230-bb0c-1df63f7bbc79"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Single Decision Tree Accuracy: 1.0000\n","Bagging Classifier Accuracy:   1.0000\n"]}]},{"cell_type":"markdown","source":["Question 8: Write a Python program to:\n","\n","● Train a Random Forest Classifier\n","\n","● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n","\n","● Print the best parameters and final accuracy\n","\n","Answer 8 : To tune a Random Forest, we use GridSearchCV, which performs an \"exhaustive search.\" It creates a grid of all possible combinations of the hyperparameters you provide, trains a model for each, and uses Cross-Validation to find the winner."],"metadata":{"id":"kmB8RpdL2ahi"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load data and split\n","data = load_breast_cancer()\n","X_train, X_test, y_train, y_test = train_test_split(\n","    data.data, data.target, test_size=0.2, random_state=42\n",")\n","\n","# 2. Define the parameter grid\n","# We will test 3 values for n_estimators and 3 for max_depth (Total 9 combinations)\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20]\n","}\n","\n","# 3. Initialize GridSearchCV\n","# cv=5 means 5-fold cross-validation\n","rf = RandomForestClassifier(random_state=42)\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n","\n","# 4. Run the search\n","grid_search.fit(X_train, y_train)\n","\n","# 5. Extract best parameters and evaluate\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","final_accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Best Parameters found: {best_params}\")\n","print(f\"Final Accuracy on Test Set: {final_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zz6e2eCY2-oD","executionInfo":{"status":"ok","timestamp":1767762603187,"user_tz":-330,"elapsed":17273,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"36bab826-43a8-48df-9807-fbd9ec9bacab"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters found: {'max_depth': None, 'n_estimators': 200}\n","Final Accuracy on Test Set: 0.9649\n"]}]},{"cell_type":"markdown","source":["Question 9: Write a Python program to:\n","\n","● Train a Bagging Regressor and a Random Forest Regressor on the California\n","Housing dataset\n","\n","● Compare their Mean Squared Errors (MSE)\n","\n","Answer 9 : To compare these two models, we will use the California Housing dataset, which is a standard regression task where the goal is to predict the median house value.\n","\n","While both models use bagging, the Random Forest Regressor is generally more robust because it adds \"feature randomness\" (selecting a random subset of features at each split) to the standard bagging process."],"metadata":{"id":"APh0TN6i3ScR"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# 1. Load the California Housing dataset\n","housing = fetch_california_housing()\n","X, y = housing.data, housing.target\n","\n","# 2. Split the data (80% training, 20% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 3. Train a Bagging Regressor\n","# Using 100 Decision Trees as base learners\n","bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(),\n","                                 n_estimators=100,\n","                                 random_state=42)\n","bagging_model.fit(X_train, y_train)\n","bagging_pred = bagging_model.predict(X_test)\n","bagging_mse = mean_squared_error(y_test, bagging_pred)\n","\n","# 4. Train a Random Forest Regressor\n","# Using 100 trees\n","rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","rf_pred = rf_model.predict(X_test)\n","rf_mse = mean_squared_error(y_test, rf_pred)\n","\n","# 5. Compare Mean Squared Errors\n","print(f\"Bagging Regressor MSE:    {bagging_mse:.4f}\")\n","print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"In5_j_bt4am7","executionInfo":{"status":"ok","timestamp":1767763029019,"user_tz":-330,"elapsed":45173,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"bec01a64-c5e1-43bd-e8ca-5bdd5f509efa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor MSE:    0.2559\n","Random Forest Regressor MSE: 0.2554\n"]}]},{"cell_type":"markdown","source":["Question 10: You are working as a data scientist at a financial institution to predict loan\n","default. You have access to customer demographic and transaction history data.\n","\n","You decide to use ensemble techniques to increase model performance.\n","\n","Explain your step-by-step approach to:\n","\n","● Choose between Bagging or Boosting\n","\n","● Handle overfitting\n","\n","● Select base models\n","\n","● Evaluate performance using cross-validation\n","\n","● Justify how ensemble learning improves decision-making in this real-world\n","context.\n","\n","Answer 10 : 1. Choosing Between Bagging and Boosting\n","\n","For loan defaults, I would likely choose Boosting (specifically algorithms like XGBoost or LightGBM).\n","\n","* Why Boosting? Financial datasets are often \"imbalanced\" (most people don't default). Boosting is superior here because it focuses on the \"hard\" cases—sequential models learn specifically from the mistakes of previous models, making them better at capturing the subtle patterns of the minority class (defaulters).\n","\n","* Why not Bagging? While Random Forest (Bagging) is very robust, it might not capture the complex, non-linear relationships in transaction history as precisely as a gradient-boosted ensemble.\n","\n","2. Handling Overfitting\n","\n","Ensemble models can easily \"memorize\" noise in transaction history. To prevent this, I would:\n","\n","* Limit Tree Depth: Use max_depth to prevent trees from growing too deep and complex.2Subsampling: Use subsample (row sampling) and colsample_bytree (feature sampling) to ensure no single transaction type or customer segment dominates the model.\n","\n","* Early Stopping: Monitor the model's performance on a validation set and stop training as soon as the validation error starts to rise, even if the training error is still falling.\n","\n","* Regularization: Apply 5$L1$ (Lasso) or 6$L2$ (Ridge) penalties to the weights of the leaves to keep them small.\n","\n","3. Selecting Base Models\n","\n","\n","* Weak Learners:I would use Decision Trees as the base learners. They handle a mix of categorical data (demographics like \"Marital Status\") and numerical data (transaction amounts) without requiring extensive scaling.\n","\n","* Diversity: If using Stacking, I might combine a Gradient Boosted Tree with a Logistic Regression model. The tree captures non-linearities, while the Logistic Regression provides a stable baseline for linear relationships between income and debt.\n","\n","4. Evaluating Performance using Cross-Validation\n","\n","I would implement Stratified K-Fold Cross-Validation ($K=5$ or $10$).\n","\n","* Stratification: This is crucial because loan defaults are rare. Stratification ensures that each \"fold\" (subset) of data has the same percentage of defaulters as the original dataset.\n","* Metric: Instead of simple Accuracy, I would evaluate based on the F1-Score or Precision-Recall AUC, ensuring the model is actually effective at identifying defaults rather than just guessing \"no default\" for everyone.\n","\n","5. Justifying Ensemble Learning in this Context\n","\n","Ensemble learning improves financial decision-making in three ways:\n","\n","* Risk Mitigation: By combining multiple models, we reduce the \"idiosyncratic error\" of any single algorithm. A single tree might find a weird correlation in one zip code; an ensemble requires that pattern to be verified across many perspectives.\n","\n","* Handling Non-Linearity: Loan default is rarely caused by one factor. It’s the interaction of demographics (age) and sudden shifts in transaction history (large withdrawals). Ensembles are naturally gifted at capturing these multi-factor interactions.\n","\n","* Stability: Financial institutions require \"stable\" models. Bagging and Boosting provide a \"consensus\" prediction that is less likely to fluctuate wildly when new, slightly different customer data is introduced next month.\n"],"metadata":{"id":"8nhPHr8748IC"}}]}