{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlsT7UB/NHmqUQq4OnKgPT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#ASSIGNMENT:\n","\n","#Supervised Classification: Decision\n","#Trees, SVM, and Naive Bayes\n","\n","\n","Question 1 : What is Information Gain, and how is it used in Decision Trees?\n","\n","Answer 1 : In the world of machine learning, Information Gain (IG) is the \"yardstick\" used by decision tree algorithms (like ID3) to determine which feature is best to split the data at any given node.\n","\n","Think of it as a measure of how much a specific question—like \"Is the weather sunny?\"—helps clear up the confusion about the final answer.\n","\n","1. The Relationship with Entropy\n","To understand Information Gain, you first need to understand Entropy. Entropy is a measure of \"disorder\" or \"impurity\" in a dataset.\n","\n","* High Entropy: The data is a messy mix (e.g., 50% \"Yes\" and 50% \"No\"). You have no idea what the outcome will be.\n","\n","* Low Entropy: The data is pure (e.g., 100% \"Yes\"). You are certain about the outcome.\n","\n","Information Gain is simply the reduction in entropy after you split the data based on a specific attribute.\n","\n","* How Decision Trees Use It\n","When a decision tree is being \"trained,\" it follows these steps:\n","\n","1) Calculate Entropy for the target variable in the current dataset.\n","\n","2) Calculate Information Gain for every available feature (e.g., Age, Income, Location).\n","\n","3) Compare results: The algorithm picks the feature with the highest Information Gain to be the decision node.\n","\n","4) Split and Repeat: The data is split into subsets based on that feature, and the process repeats for each branch until the data is \"pure\" or a stopping condition is met.\n","\n","\n","Question 2: What is the difference between Gini Impurity and Entropy?\n","\n","Answer 2 : Both Gini Impurity and Entropy are metrics used by Decision Trees to decide where to \"split\" the data. While they often lead to very similar results, they have different mathematical origins and slight behavioral differences.\n","\n","**Gini Impurity**\n","\n","* Strengths: Its main advantage is efficiency.3 Because it uses squared probabilities (4$1 - \\sum p_i^2$), it is much easier for a computer to process, especially on massive datasets with millions of rows.\n","\n","* Weaknesses: It can sometimes be \"lazy.\" It tends to isolate the most frequent class into its own branch, which might result in a slightly less balanced tree in complex scenarios.\n","\n","* Best Use Case: Large datasets where speed is a priority, or as a general-purpose default.\n","\n","**Entropy**\n","\n","* Strengths: It is more sensitive to small changes in class probabilities. Because the log function grows steeply near zero, Entropy penalizes \"impurity\" more harshly than Gini does. This often leads to more balanced trees.\n","\n","\n","* Weaknesses: Computational cost. Calculating logarithms for every single potential split at every node takes more time and battery/CPU power.\n","\n","*  Best Use Case: Smaller datasets where you want the most \"informative\" split possible, or when you are performing exploratory data analysis and want a deeper look at uncertainty.\n","\n","Question 3:What is Pre-Pruning in Decision Trees?\n","\n","Answer 3 : Pre-Pruning is the \"stop early\" rule for a decision tree.\n","\n","When a decision tree grows, it naturally wants to keep splitting until every single data point is perfectly categorized. This often leads to a tree that is too complex, messy, and \"overfitted\" (meaning it memorizes the training data but fails to predict new data correctly).\n","\n","Pre-Pruning prevents this by stopping the tree's growth before it becomes too complicated.\n","\n","* How it Works:\n","\n","Think of it like a gardener trimming a hedge as it grows, rather than waiting for it to become a wild mess and cutting it back later. You set specific \"stopping rules\" at the start. If a node doesn't meet these rules, the tree stops splitting that branch.\n","\n","Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n","Impurity as the criterion and print the feature importances (practical).\n","\n","Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n","\n","(Include your Python code and output in the code box below.)\n","\n","Answer 4 : To build a Decision Tree using Gini Impurity, we use the scikit-learn library. In this example, I'll use the classic Iris Dataset (flower species) because it is built into the library and easy to visualize.\n","\n","* Python Implementation\n","\n","\n"],"metadata":{"id":"m2bY7dBxiPW-"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","import pandas as pd\n","\n","# 1. Load the dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","feature_names = iris.feature_names\n","\n","# 2. Initialize the Classifier\n","# We specify 'gini' as the criterion (though it is the default)\n","clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n","\n","# 3. Train (fit) the model\n","clf.fit(X, y)\n","\n","# 4. Get Feature Importances\n","importances = clf.feature_importances_\n","\n","# 5. Create a clean output\n","feature_importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance Score': importances\n","}).sort_values(by='Importance Score', ascending=False)\n","\n","print(\"Decision Tree Feature Importances (Gini):\")\n","print(feature_importance_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jof0BJjcpW8P","executionInfo":{"status":"ok","timestamp":1767423468123,"user_tz":-330,"elapsed":48,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"a5f35cc5-470a-42a8-8579-b3f9bf8b04f8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Feature Importances (Gini):\n","             Feature  Importance Score\n","2  petal length (cm)          0.564056\n","3   petal width (cm)          0.422611\n","0  sepal length (cm)          0.013333\n","1   sepal width (cm)          0.000000\n"]}]},{"cell_type":"markdown","source":["Question 5: What is a Support Vector Machine (SVM)?\n","\n","Answer 5 : A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification (sorting data into categories) and regression (predicting numerical values).\n","\n","* The Core Components:\n","\n","To find the best boundary, an SVM uses three key concepts:\n","\n","1) Hyperplane: This is the actual decision boundary (the line) that separates the classes. In 2D, it's a line; in 3D, it's a plane; and in higher dimensions, it’s called a \"hyperplane.\"\n","\n","\n","2) Support Vectors: These are the data points located closest to the boundary. They are the most critical points because if you moved them, the boundary would move. They \"support\" the hyperplane.\n","\n","\n","\n","3) Margin: This is the \"no-man's land\" or the gap between the hyperplane and the support vectors. SVM tries to make this gap as wide as possible to ensure the classes are clearly separated.\n","\n","Question 6: What is the Kernel Trick in SVM?\n","\n","Answer 6 : The \"Kernel Trick\"\n","\n","Real-world data is rarely perfectly separable by a straight line. Sometimes data points are mixed in a way that requires a curved boundary.\n","\n","* The Problem: In 2D space, the data looks like a messy jumble.\n","\n","* The Solution: SVM uses a Kernel to mathematically \"lift\" the data into a higher dimension (like adding a 3rd dimension).\n","\n","* The Result: In this higher dimension, the data becomes separable by a straight plane. When you project that plane back down to 2D, it looks like a perfect circle or curve.\n","\n","Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n","\n","kernels on the Wine dataset, then compare their accuracies.\n","\n","Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n","on the same dataset.\n","(Include your Python code and output in the code box below.)\n","\n","Answer 7 : To compare the performance of different SVM kernels, we use the Wine dataset, which contains chemical analysis results of wines grown in the same region in Italy.\n","\n","* Python Implementation"],"metadata":{"id":"yzTCe1jjpfeN"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Wine dataset\n","wine = load_wine()\n","X, y = wine.data, wine.target\n","\n","# 2. Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 3. Train SVM with Linear Kernel\n","linear_svc = SVC(kernel='linear', random_state=42)\n","linear_svc.fit(X_train, y_train)\n","linear_preds = linear_svc.predict(X_test)\n","linear_accuracy = accuracy_score(y_test, linear_preds)\n","\n","# 4. Train SVM with RBF (Radial Basis Function) Kernel\n","rbf_svc = SVC(kernel='rbf', random_state=42)\n","rbf_svc.fit(X_train, y_train)\n","rbf_preds = rbf_svc.predict(X_test)\n","rbf_accuracy = accuracy_score(y_test, rbf_preds)\n","\n","# 5. Compare the results\n","print(f\"Accuracy with Linear Kernel: {linear_accuracy:.4f}\")\n","print(f\"Accuracy with RBF Kernel: {rbf_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fO2Mu4Vqqh-3","executionInfo":{"status":"ok","timestamp":1767423782089,"user_tz":-330,"elapsed":495,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"a304234a-c498-4670-befb-ce35eea099e7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Linear Kernel: 0.9815\n","Accuracy with RBF Kernel: 0.7593\n"]}]},{"cell_type":"markdown","source":["Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n","\n","Answer 8 : The Naïve Bayes classifier is a popular machine learning algorithm used primarily for classification tasks, such as sorting emails into \"Spam\" or \"Not Spam.\" It is based on the Bayes’ Theorem, a mathematical formula used to calculate the probability of an event based on prior knowledge.\n","\n","**Why is it called \"Naïve\"?**\n","\n","The \"Naïve\" part comes from a very bold (and usually unrealistic) assumption the algorithm makes: Independence.\n","\n","The algorithm assumes that every feature is completely unrelated to every other feature.\n","\n","* In a \"Non-Naïve\" world: If you see the word \"Credit,\" there is a high chance the next word is \"Card.\" The words are dependent on each other.\n","\n","* In the \"Naïve\" world: The algorithm treats \"Credit\" and \"Card\" as if they have absolutely nothing to do with each other. It ignores the context and the relationship between features.\n","\n","Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n","Bayes, and Bernoulli Naïve Bayes\n","\n","Answer 9 : While all three algorithms use Bayes' Theorem, they are designed for different types of data. The main difference lies in the distribution (the shape or pattern) of the features you are feeding into the model.\n","\n","**1. Gaussian Naïve Bayes (GNB)**:\n","\n","This version is used when your features are continuous (numbers that can have decimals) and follow a Normal (Gaussian) Distribution (the \"bell curve\").\n","\n","* Data Type: Real-world measurements like height, weight, temperature, or blood pressure.\n","\n","* How it works: It calculates the mean and standard deviation of your features to estimate probabilities.\n","\n","Example: Predicting if a person is an athlete based on their height and weight.\n","\n","**2. Multinomial Naïve Bayes (MNB)**:\n","\n","This is the \"go-to\" algorithm for text classification. it is used when your data represents counts or frequencies.\n","\n","* Data Type: Discrete counts (integers). In text, this is usually the \"Word Count\" (how many times the word \"Win\" appears in an email).\n","\n","* How it works: It looks at the frequency of events. It doesn't care just if a word is there, but how many times it appears.\n","\n","Example: Sorting news articles into categories like \"Sports,\" \"Politics,\" or \"Tech\" based on word frequencies.\n","\n","**3. Bernoulli Naïve Bayes (BNB)**:\n","\n","This version is used when your features are binary (Yes/No, 1/0). It only cares about whether a feature exists or not.\n","\n","* Data Type: Boolean values. In text, this is \"Presence vs. Absence\" (is the word \"Winner\" in this email? Yes or No).\n","\n","* How it works: It ignores how many times a word appears. It treats 1 occurrence the same as 10 occurrences.\n","\n","Example: Simple spam filters where the mere presence of a \"trigger word\" is enough to flag the message, regardless of its frequency.\n","\n","Question 10: Breast Cancer Dataset\n","\n","Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n","dataset and evaluate accuracy.\n","\n","Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n","sklearn.datasets.\n","\n","(Include your Python code and output in the code box below.)\n","\n"],"metadata":{"id":"2WvG6e07qyo-"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# 2. Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 3. Initialize the Gaussian Naive Bayes classifier\n","model = GaussianNB()\n","\n","# 4. Train the model\n","model.fit(X_train, y_train)\n","\n","# 5. Make predictions and evaluate accuracy\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Accuracy on Breast Cancer Dataset: {accuracy:.4f}\")"],"metadata":{"id":"q6gi_pM6slNk","executionInfo":{"status":"ok","timestamp":1767424287808,"user_tz":-330,"elapsed":65,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"578fab98-1b78-4d3f-a866-49aed60cd786","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on Breast Cancer Dataset: 0.9737\n"]}]}]}